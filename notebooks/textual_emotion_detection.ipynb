{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Importing libraries and loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30213/1870993683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re\n",
    "import string\n",
    "from transformers import TFBertModel, BertTokenizerFast, BertConfig\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.initializers import TruncatedNormal\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Plot Defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('rocket_r')\n",
    "sns.set_palette('rocket_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def idx2class(idx_list):\n",
    "    \"\"\"\n",
    "    This function converts a list of class indices to a list of class labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idx_list : list\n",
    "        List of class indices.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    class_list : list\n",
    "        List of class labels.\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    for i in idx_list:\n",
    "        arr.append(labels[int(i)])\n",
    "    return arr\n",
    "\n",
    "def EmotionMapping(list_of_emotions):\n",
    "    list = []  \n",
    "    for i in list_of_emotions:\n",
    "        if i in ekman_map['anger']:\n",
    "            list.append('anger')\n",
    "        if i in ekman_map['disgust']:\n",
    "            list.append('disgust')\n",
    "        if i in ekman_map['fear']:\n",
    "            list.append('fear')\n",
    "        if i in ekman_map['joy']:\n",
    "            list.append('joy')\n",
    "        if i in ekman_map['sadness']:\n",
    "            list.append('sadness')\n",
    "        if i in ekman_map['surprise']:\n",
    "            list.append('surprise')\n",
    "        if i == 'neutral':\n",
    "            list.append('neutral')      \n",
    "    return list\n",
    "\n",
    "def SentimentMapping(list_of_emotions):\n",
    "    list = []  \n",
    "    for i in list_of_emotions:\n",
    "        if i in sentiment_map['positive']:\n",
    "            list.append('positive')\n",
    "        if i in sentiment_map['negative']:\n",
    "            list.append('negative')\n",
    "        if i in sentiment_map['ambiguous']:\n",
    "            list.append('ambiguous')\n",
    "    return list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_url = 'https://github.com/google-research/google-research/raw/master/goemotions/data/train.tsv'\n",
    "valid_url = 'https://github.com/google-research/google-research/raw/master/goemotions/data/dev.tsv'\n",
    "test_url = 'https://github.com/google-research/google-research/raw/master/goemotions/data/test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_url, sep='\\t', encoding='utf-8',\n",
    "                       names=['text', 'emotion', 'annotator'], header=None)\n",
    "valid_df = pd.read_csv(valid_url, sep='\\t', encoding='utf-8',\n",
    "                       names=['text', 'emotion', 'annotator'], header=None)\n",
    "test_df = pd.read_csv(test_url, sep='\\t', encoding='utf-8',\n",
    "                      names=['text', 'emotion', 'annotator'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.5 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Column 2 \"annotator\" is unnecessary, so we can drop it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df.drop('annotator', axis=1, inplace=True)\n",
    "valid_df.drop('annotator', axis=1, inplace=True)\n",
    "test_df.drop('annotator', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dictionaries for mapping emotions to indices and vice versa. \n",
    "\n",
    "The variable `ekman_map` is used to map 27 emotions to 7 emotions. This is done to reduce the number of classes.\n",
    "\n",
    "The 27 emotions can also be mapped to the 3 emotions using the `sentiment_map` dictionary for sentiment analysis tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0: 'admiration',\n",
    "    1: 'amusement',\n",
    "    2: 'anger',\n",
    "    3: 'annoyance',\n",
    "    4: 'approval',\n",
    "    5: 'caring',\n",
    "    6: 'confusion',\n",
    "    7: 'curiosity',\n",
    "    8: 'desire',\n",
    "    9: 'disappointment',\n",
    "    10: 'disapproval',\n",
    "    11: 'disgust',\n",
    "    12: 'embarrassment',\n",
    "    13: 'excitement',\n",
    "    14: 'fear',\n",
    "    15: 'gratitude',\n",
    "    16: 'grief',\n",
    "    17: 'joy',\n",
    "    18: 'love',\n",
    "    19: 'nervousness',\n",
    "    20: 'optimism',\n",
    "    21: 'pride',\n",
    "    22: 'realization',\n",
    "    23: 'relief',\n",
    "    24: 'remorse',\n",
    "    25: 'sadness',\n",
    "    26: 'surprise',\n",
    "    27: 'neutral'\n",
    "}\n",
    "\n",
    "ekman_map = {\n",
    "    'anger': ['anger', 'annoyance', 'disapproval'],\n",
    "    'disgust': ['disgust'],\n",
    "    'fear': ['fear', 'nervousness'],\n",
    "    'joy': ['joy', 'amusement', 'approval', 'excitement', 'gratitude',  'love', 'optimism', 'relief', 'pride', 'admiration', 'desire', 'caring'],\n",
    "    'sadness': ['sadness', 'disappointment', 'embarrassment', 'grief',  'remorse'],\n",
    "    'surprise': ['surprise', 'realization', 'confusion', 'curiosity'],\n",
    "    'neutral': ['neutral']\n",
    "}\n",
    "\n",
    "sentiment_map = {\n",
    "    \"positive\": [\"amusement\", \"excitement\", \"joy\", \"love\", \"desire\", \"optimism\", \"caring\", \"pride\", \"admiration\", \"gratitude\", \"relief\", \"approval\"],\n",
    "    \"negative\": [\"fear\", \"nervousness\", \"remorse\", \"embarrassment\", \"disappointment\", \"sadness\", \"grief\", \"disgust\", \"anger\", \"annoyance\", \"disapproval\"],\n",
    "    \"ambiguous\": [\"realization\", \"surprise\", \"curiosity\", \"confusion\", \"neutral\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, let's extract all emotions from the each example and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df['list of emotions'] = train_df['emotion'].apply(lambda x: x.split(','))\n",
    "test_df['list of emotions'] = test_df['emotion'].apply(lambda x: x.split(','))\n",
    "valid_df['list of emotions'] = valid_df['emotion'].apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can then apply index to class mapping to get the class labels for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df['emotion'] = train_df['list of emotions'].apply(lambda x: idx2class(x))\n",
    "test_df['emotion'] = test_df['list of emotions'].apply(lambda x: idx2class(x))\n",
    "valid_df['emotion'] = valid_df['list of emotions'].apply(lambda x: idx2class(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, we can reduce the number of classes to 7 by using the EmotionMapping function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df['ekman_emotion'] = train_df['emotion'].apply(lambda x: EmotionMapping(x))\n",
    "test_df['ekman_emotion'] = test_df['emotion'].apply(lambda x: EmotionMapping(x))\n",
    "valid_df['ekman_emotion'] = valid_df['emotion'].apply(lambda x: EmotionMapping(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function cleans the text in the dataframe and returns a list of cleaned text.\n",
    "    text: a string\n",
    "\n",
    "    return: modified initial string\n",
    "    \"\"\"\n",
    "    # Removing Emojis\n",
    "    text = emoji.demojize(text)  # remove emojis\n",
    "    text = str(text).lower()  # text to lower case\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)  # remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One hot encoding of emotions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in ekman_map:\n",
    "    train_df[i] = train_df['ekman_emotion'].apply(lambda x: 1 if i in x else 0)\n",
    "    test_df[i] = test_df['ekman_emotion'].apply(lambda x: 1 if i in x else 0)\n",
    "    valid_df[i] = valid_df['ekman_emotion'].apply(lambda x: 1 if i in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.6 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Bar plot of distribution of emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_summary = train_df.iloc[:, 4:].sum()\n",
    "labels_summary.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=labels_summary.index,\n",
    "            y=labels_summary.values, palette='rocket_r')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Number of emotions in each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df['n_emotions'] = train_df.iloc[:, 4:].apply(lambda x: x.sum(), axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='n_emotions', data=train_df, palette='rocket_r')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of emotions per sample')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Number of emotions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Distribution of text length in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_text = pd.concat([train_df['text'], valid_df['text'], test_df['text']])\n",
    "lengths = full_text.apply(lambda x: len(x.split()))\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "sns.displot(lengths, kde=True, rug=False, color=colors[5])\n",
    "plt.title('Distribution of Text Lengths')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Base model config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Computing max length of samples\n",
    "\n",
    "`max_length` variable is used to limit the length of the input text that is fed to the model. The sequence will be padded with the `<PAD>` token if the length of the sequence is less than `max_length` and the sequence will be truncated if the length of the sequence is more than `max_length`. This is done to ensure that the model can handle any size of input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_text = pd.concat([train_df['text'], valid_df['text'], test_df['text']])\n",
    "max_length = full_text.apply(lambda x: len(x.split())).max()\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I am going to use Google's BERT base model which contains 110M parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "config = BertConfig.from_pretrained(model_name, output_hidden_states=False)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n",
    "transformer_model = TFBertModel.from_pretrained(model_name, config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "model takes three inputs that result from tokenization:\n",
    "\n",
    "- `input_ids`: indices of input sequence tokens in the vocabulary\n",
    "- `token_type_ids`: Segment token indices to indicate first and second portions of the inputs. 0 for sentence A and 1 for sentence B\n",
    "- `attention mask`: Mask to avoid performing attention on padding token indices. 0 for masked and 1 for not masked\n",
    "\n",
    "I have a sigmoided output layer in the model because it is more appropriate than a softmax layer. This is because I are trying to predict the probability of each label and not the label itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def my_model(n_labels):\n",
    "\n",
    "  # Load the MainLayer\n",
    "  bert = transformer_model.layers[0]\n",
    "\n",
    "  ## INPUTS\n",
    "  input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "  attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32')\n",
    "  token_type_ids = Input(shape=(max_length,), name='token_type_ids', dtype='int32')\n",
    "  inputs = {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}\n",
    "\n",
    "  ## LAYERS\n",
    "  bert_model = bert(inputs)[1]\n",
    "  dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
    "  pooled_output = dropout(bert_model, training=False)\n",
    "\n",
    "  ## OUTPUT\n",
    "  emotion = Dense(units=n_labels, activation='sigmoid', kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='emotion')(pooled_output)\n",
    "  outputs = emotion\n",
    "\n",
    "  model = Model(inputs=inputs, outputs=outputs, name='BERT_Emotion_Classifier')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = my_model(len(ekman_map))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 Data tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Train \n",
    "x_train = train_df['text']\n",
    "y_train = train_df.loc[:, ekman_map.keys()].values\n",
    "\n",
    "train_tokenized = tokenizer(\n",
    "    text = list(x_train),\n",
    "    add_special_tokens = True,\n",
    "    max_length = max_length,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'tf',\n",
    "    return_attention_mask = True,\n",
    "    return_token_type_ids = True\n",
    ")\n",
    "\n",
    "## Test\n",
    "x_test = test_df['text']\n",
    "y_test = test_df.loc[:, ekman_map.keys()].values\n",
    "\n",
    "test_tokenized = tokenizer(\n",
    "    text = list(x_test),\n",
    "    add_special_tokens = True,\n",
    "    max_length = max_length,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'tf',\n",
    "    return_attention_mask = True,\n",
    "    return_token_type_ids = True\n",
    ")\n",
    "\n",
    "## Validation\n",
    "x_valid = valid_df['text']\n",
    "y_valid = valid_df.loc[:, ekman_map.keys()].values\n",
    "\n",
    "valid_tokenized = tokenizer(\n",
    "    text = list(x_valid),\n",
    "    add_special_tokens = True,\n",
    "    max_length = max_length,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'tf',\n",
    "    return_attention_mask = True,\n",
    "    return_token_type_ids = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 Creating BERT compatible inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf_train = {'input_ids': train_tokenized['input_ids'], 'attention_mask': train_tokenized['attention_mask'], 'token_type_ids': train_tokenized['token_type_ids']}\n",
    "tf_test = {'input_ids': test_tokenized['input_ids'], 'attention_mask': test_tokenized['attention_mask'], 'token_type_ids': test_tokenized['token_type_ids']}\n",
    "tf_valid = {'input_ids': valid_tokenized['input_ids'], 'attention_mask': valid_tokenized['attention_mask'], 'token_type_ids': valid_tokenized['token_type_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.from_tensor_slices((tf_train, y_train)).batch(80)\n",
    "valid = tf.data.Dataset.from_tensor_slices((tf_valid, y_valid)).batch(80)\n",
    "test = tf.data.Dataset.from_tensor_slices((tf_test, y_test)).batch(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=5e-5,\n",
    "    decay_rate=0.7,\n",
    "    decay_steps=340,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prior experiments with BERT showed that the model starts to overfit after ~2 epochs and Tanh performed significantly worse than sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "history = model.fit(train, epochs=2, validation_data=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights('./models/sigmoid_bert.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When dealing with unbalanced data, it is essential to mini-batch train the model instead of training it on all the data. This helps to prevent the model from overfitting the minority class. It is also essential to be thoughtful about what metric is being used for model evaluation. When dealing with unbalanced data, accuracy is not a good metric, as the model can predict the majority class every time and still have high accuracy. Instead, it is crucial to use the precision/recall or the F1 score, as these metrics consider false positives and false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = my_model(len(ekman_map))\n",
    "model.load_weights('./models/sigmoid_bert.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.83\n",
    "\n",
    "y_pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "probabilities = y_pred\n",
    "\n",
    "probabilities = pd.DataFrame(probabilities, columns=ekman_map.keys())\n",
    "probabilities.index = x_test\n",
    "probabilities.reset_index(inplace=True)\n",
    "probabilities.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred > THRESHOLD, 1, 0)\n",
    "\n",
    "recall = []\n",
    "f1 = []\n",
    "precision = []\n",
    "emotions = ekman_map.keys()\n",
    "\n",
    "for i in range(len(emotions)):\n",
    "    f1.append(f1_score(y_test[:, i], y_pred[:, i], average='macro'))\n",
    "    precision.append(precision_score(y_test[:, i], y_pred[:, i], average='macro'))\n",
    "\n",
    "results = pd.DataFrame({'precision': precision, 'f1': f1})\n",
    "results.index = emotions\n",
    "\n",
    "means = {'precision': np.mean(precision), 'f1': np.mean(f1)}\n",
    "means = pd.DataFrame(means, index=['mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([results, means], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finding the best value of Threshold. I chose f1-score as the main metric because it is more robust than precision and recall alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "pred = model.predict(test)\n",
    "\n",
    "for threshold in np.arange(0.30, 0.99, 0.01):\n",
    "    preds = np.where(pred > threshold, 1, 0)\n",
    "\n",
    "    f1 = f1_score(y_test, preds, average='macro', zero_division=0)\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_threshold = threshold\n",
    "        best_f1 = f1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Best threshold: {best_threshold}\\nBest f1: {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pred(text, model, THRESHOLD):\n",
    "\n",
    "    text = [clean_text(text) for text in text]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        text = text,\n",
    "        add_special_tokens = True,\n",
    "        max_length = max_length,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        return_tensors = 'tf',\n",
    "        return_attention_mask = True,\n",
    "        return_token_type_ids = True\n",
    "    )\n",
    "\n",
    "    tf_test = {'input_ids': tokenized['input_ids'], 'attention_mask': tokenized['attention_mask'], 'token_type_ids': tokenized['token_type_ids']}\n",
    "\n",
    "    pred = model.predict(tf_test)\n",
    "\n",
    "    probabilities = pred\n",
    "    probabilities = pd.DataFrame(probabilities, columns=ekman_map.keys())\n",
    "    probabilities.index = text\n",
    "    probabilities.reset_index(inplace=True)\n",
    "\n",
    "    pred = np.where(pred > THRESHOLD, 1, 0)\n",
    "\n",
    "    pred = pd.DataFrame(pred, columns=ekman_map.keys())\n",
    "    pred['emotion'] = pred.iloc[:, 1:].idxmax(axis=1)\n",
    "    pred.drop(columns=emotions, inplace=True)\n",
    "    pred.index = text\n",
    "    pred.reset_index(inplace=True)\n",
    "\n",
    "    return pred, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result, probabilities = pred(['A Ukrainian woman who escaped Russias assault on Mariupol says troops were targeting apartment buildings as if they were playing a computer game', 'I often go to parks to walk and destress and enjoy nature', 'How can this be', 'This is the worst muffin ive ever had'], model, THRESHOLD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fec9c6b2ce58022ac7011e59ab821d27b954e64ab27f1573ace158a2ddd131d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
